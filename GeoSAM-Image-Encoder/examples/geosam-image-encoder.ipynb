{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "1nlvcmIyizdxZuJ66IapWW-RX1PlN4stW",
      "authorship_tag": "ABX9TyNMuxAe6UZmVljFb3gKU0cp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/Fanchengyan/291a271f44a572cb28e7b79ece7c950f/geosam-image-encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GeoSAM-Image-Encoder\n"
      ],
      "metadata": {
        "id": "ofyKNeqP6Wj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "Installing `GeoSAM-Image-Encoder` directly will install the CPU version of `PyTorch`. Therefore, it is recommended to install the appropriate version of `PyTorch` before installing `GeoSAM-Image-Encoder` in your machine. You can install the corresponding version based on the official PyTorch website:\n",
        "<https://pytorch.org/get-started/locally/>\n",
        "\n",
        "After installing PyTorch, you can install `GeoSAM-Image-Encoder` via pip.\n"
      ],
      "metadata": {
        "id": "yd7SYS1o6NTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Colab, PyTorch is already built-in, so you can install it directly."
      ],
      "metadata": {
        "id": "dM0YGfCtsfUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_LOrmGt3Gkh"
      },
      "outputs": [],
      "source": [
        "!pip install GeoSAM-Image-Encoder\n",
        "# or\n",
        "# !pip install git+https://github.com/Fanchengyan/GeoSAM-Image-Encoder.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download example dataset and sam `vit_l` checkpoint"
      ],
      "metadata": {
        "id": "dM9ztEAm62Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/coolzhao/Geo-SAM/main/rasters/beiluhe_google_img_201211_clip.tif\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
        "!wget https://raw.githubusercontent.com/coolzhao/Geo-SAM/dev/GeoSAM-Image-Encoder/examples/data/setting.json"
      ],
      "metadata": {
        "id": "v32Lb6YW5FNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage"
      ],
      "metadata": {
        "id": "ILKiN60dXhQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Python\n",
        "\n",
        "After install GeoSAM-Image-Encoder, you can import it using `geosam`"
      ],
      "metadata": {
        "id": "96Dof82l31rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geosam\n",
        "from geosam import ImageEncoder"
      ],
      "metadata": {
        "id": "Z0K8RQV63H_v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check the folder contains geosam and add it to environment if you want to run in terminal"
      ],
      "metadata": {
        "id": "34LNsf9H8oIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "geosam.folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LKQcboGt7US4",
        "outputId": "6973806f-77b6-46b5-8a99-8babfe14aab2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/geosam'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if gpu available\n",
        "geosam.gpu_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwvXGZLYS5LZ",
        "outputId": "9647f83c-7416-4dae-dd2a-5ca0a762b979"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run by specify parameters dirctly"
      ],
      "metadata": {
        "id": "gb03VNJe4O2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We recommend using Python interface directly for processing, which will have greater flexibility."
      ],
      "metadata": {
        "id": "bXjV25Hg9pNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = '/content/sam_vit_l_0b3195.pth'\n",
        "image_path = '/content/beiluhe_google_img_201211_clip.tif'\n",
        "feature_dir = './'\n",
        "\n",
        "## init ImageEncoder\n",
        "img_encoder = ImageEncoder(checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNwD3v3D8RD1",
        "outputId": "f892c473-e9a1-42d5-ae75-c5c1bcd9261f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing SAM model...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## encode image\n",
        "img_encoder.encode_image(image_path,feature_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3peY4H_Y90So",
        "outputId": "bea1cadf-add4-440e-e75e-d71e570bf72f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------\n",
            "     Start encoding image to SAM features\n",
            "----------------------------------------------\n",
            "\n",
            "Input Parameters:\n",
            "----------------------------------------------\n",
            " Input data value range to be rescaled: [0, 255] (automatically set based on min-max value of input image inside the processing extent.)\n",
            " Image path: /content/beiluhe_google_img_201211_clip.tif\n",
            " Bands selected: ['1', '2', '3']\n",
            " Target resolution: 0.9999395530145561\n",
            " Processing extent: [471407.9709, 3882162.2353, 473331.8546, 3884389.1008]\n",
            " Processing image size: (width 1924, height 2227)\n",
            "----------------------------------------------\n",
            "\n",
            "\n",
            "RasterDataset info \n",
            "----------------------------------------------\n",
            " filename_glob: beiluhe_google_img_201211_clip.tif, \n",
            " all bands: ['1', '2', '3', '4'], \n",
            " input bands: ['1', '2', '3'], \n",
            " resolution: 0.9999395530145561, \n",
            " bounds: [471407.9709, 473331.8546571067, 3882162.2353493366, 3884389.1008, 0.0, 9.223372036854776e+18], \n",
            " num: 1\n",
            "----------------------------------------------\n",
            "\n",
            "----------------------------------------------\n",
            " SAM model initialized. \n",
            "  SAM model type:  vit_l\n",
            " Device type: cuda:0\n",
            " Patch size: (1024, 1024) \n",
            " Batch size: 1\n",
            " Patch sample num: 12\n",
            " Total batch num: 12\n",
            "----------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding image: 100%|██████████| 12/12 [00:19<00:00,  1.61s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Output feature path\": .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run by parameters from setting.json file\n",
        "\n",
        "If you want to using settings.json file to provide the parameters"
      ],
      "metadata": {
        "id": "SuHYf5BQTT1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setting_file = \"/content/setting.json\"\n",
        "feature_dir = './'\n",
        "\n",
        "### parse settings from the setting,json file\n",
        "settings = geosam.parse_settings_file(setting_file)\n",
        "\n",
        "### setting file not contains feature_dir, you need add it\n",
        "settings.update({\"feature_dir\":feature_dir})\n",
        "\n",
        "### split settings into init_settings, encode_settings\n",
        "init_settings, encode_settings = geosam.split_settings(settings)\n",
        "\n",
        "print(f\"settings: {settings}\")\n",
        "print(f\"init_settings: {init_settings}\")\n",
        "print(f\"encode_settings: {encode_settings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4r3xTViM91YL",
        "outputId": "7b9fefbf-f94c-45b0-d58f-8380be69a1b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "settings: {'image_path': '/content/beiluhe_google_img_201211_clip.tif', 'bands': [1, 1, 1], 'value_range': '0.0,255.0', 'extent': '471407.9709, 473331.8546, 3882162.2353, 3884389.1008 [EPSG:32646]', 'resolution': 0.9999395530145561, 'stride': 512, 'checkpoint_path': '/content/sam_vit_l_0b3195.pth', 'model_type': 1, 'batch_size': 1, 'gpu_id': 0, 'feature_dir': './'}\n",
            "init_settings: {'checkpoint_path': '/content/sam_vit_l_0b3195.pth', 'model_type': 1, 'batch_size': 1, 'gpu_id': 0}\n",
            "encode_settings: {'image_path': '/content/beiluhe_google_img_201211_clip.tif', 'bands': [1, 1, 1], 'value_range': '0.0,255.0', 'extent': '471407.9709, 473331.8546, 3882162.2353, 3884389.1008 [EPSG:32646]', 'resolution': 0.9999395530145561, 'stride': 512, 'feature_dir': './'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Then, you can run image incoding by parrameters from setting.json file\n",
        "img_encoder = ImageEncoder(**init_settings)\n",
        "img_encoder.encode_image(**encode_settings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7fMTVTtFyjb",
        "outputId": "5fc4cd7b-401a-4197-8d3f-e973de1b855e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing SAM model...\n",
            "\n",
            "\n",
            "----------------------------------------------\n",
            "     Start encoding image to SAM features\n",
            "----------------------------------------------\n",
            "\n",
            "Input Parameters:\n",
            "----------------------------------------------\n",
            " Input data value range to be rescaled: (0.0, 255.0) (set by user)\n",
            " Image path: /content/beiluhe_google_img_201211_clip.tif\n",
            " Bands selected: ['1', '1', '1']\n",
            " Target resolution: 0.9999395530145561\n",
            " Processing extent: (471407.9709, 473331.8546, 3882162.2353, 3884389.1008)\n",
            " Processing image size: (width 3410960, height 3411263)\n",
            "----------------------------------------------\n",
            "\n",
            "\n",
            "RasterDataset info \n",
            "----------------------------------------------\n",
            " filename_glob: beiluhe_google_img_201211_clip.tif, \n",
            " all bands: ['1', '2', '3', '4'], \n",
            " input bands: ['1', '1', '1'], \n",
            " resolution: 0.9999395530145561, \n",
            " bounds: [471407.9709, 473331.8546571067, 3882162.2353493366, 3884389.1008, 0.0, 9.223372036854776e+18], \n",
            " num: 1\n",
            "----------------------------------------------\n",
            "\n",
            "----------------------------------------------\n",
            " SAM model initialized. \n",
            "  SAM model type:  vit_l\n",
            " Device type: cuda:0\n",
            " Patch size: (1024, 1024) \n",
            " Batch size: 1\n",
            " Patch sample num: 12\n",
            " Total batch num: 12\n",
            "----------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding image: 100%|██████████| 12/12 [00:13<00:00,  1.11s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Output feature path\": .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Terminal\n",
        "\n",
        "Since this is a Colab example, Python will be used to demonstrate running it in the terminal."
      ],
      "metadata": {
        "id": "0YiFNWuz4iWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "## change cwd to geosam folder\n",
        "os.chdir(geosam.folder)\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDr4oyCKHMpV",
        "outputId": "7c86845d-006b-4be5-b2f1-b670e9510d44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/geosam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## get the command for terminal\n",
        "cmd = f\"image_encoder.py -i {image_path} -c {checkpoint_path} -f {feature_dir}\"\n",
        "print(cmd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD686kzAXR6G",
        "outputId": "d6efe171-366a-4d4a-c10a-490f59312d5b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_encoder.py -i /content/beiluhe_google_img_201211_clip.tif -c /content/sam_vit_l_0b3195.pth -f ./\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## run in terminal\n",
        "!python image_encoder.py -i /content/beiluhe_google_img_201211_clip.tif -c /content/sam_vit_l_0b3195.pth -f ./\n",
        "\n",
        "## You can overwrite the settings from file by specify the parameter values. For Example:\n",
        "# !python image_encoder.py -s /content/setting.json  -f ./ --stride 256 --value_range \"10,255\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_LbTprzXa0M",
        "outputId": "7d264629-c1a5-4c89-feec-b11736831198"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "settings:\n",
            " {'feature_dir': PosixPath('/usr/local/lib/python3.10/dist-packages/geosam'), 'image_path': PosixPath('/content/beiluhe_google_img_201211_clip.tif'), 'checkpoint_path': PosixPath('/content/sam_vit_l_0b3195.pth'), 'stride': 512, 'batch_size': 1, 'gpu_id': 0}\n",
            "\n",
            "Initializing SAM model...\n",
            "\n",
            "\n",
            "----------------------------------------------\n",
            "     Start encoding image to SAM features\n",
            "----------------------------------------------\n",
            "\n",
            "Input Parameters:\n",
            "----------------------------------------------\n",
            " Input data value range to be rescaled: [0, 255] (automatically set based on min-max value of input image inside the processing extent.)\n",
            " Image path: /content/beiluhe_google_img_201211_clip.tif\n",
            " Bands selected: ['1', '2', '3']\n",
            " Target resolution: 0.9999395530145561\n",
            " Processing extent: [471407.9709, 3882162.2353, 473331.8546, 3884389.1008]\n",
            " Processing image size: (width 1924, height 2227)\n",
            "----------------------------------------------\n",
            "\n",
            "\n",
            "RasterDataset info \n",
            "----------------------------------------------\n",
            " filename_glob: beiluhe_google_img_201211_clip.tif, \n",
            " all bands: ['1', '2', '3', '4'], \n",
            " input bands: ['1', '2', '3'], \n",
            " resolution: 0.9999395530145561, \n",
            " bounds: [471407.9709, 473331.8546571067, 3882162.2353493366, 3884389.1008, 0.0, 9.223372036854776e+18], \n",
            " num: 1\n",
            "----------------------------------------------\n",
            "\n",
            "----------------------------------------------\n",
            " SAM model initialized. \n",
            "  SAM model type:  vit_l\n",
            " Device type: cuda:0\n",
            " Patch size: (1024, 1024) \n",
            " Batch size: 1\n",
            " Patch sample num: 12\n",
            " Total batch num: 12\n",
            "----------------------------------------------\n",
            "\n",
            "Encoding image: 100% 12/12 [00:13<00:00,  1.16s/batch]\n",
            "\"Output feature path\": /usr/local/lib/python3.10/dist-packages/geosam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## check all available parameters:\n",
        "!python image_encoder.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aXYVNWVyUbg",
        "outputId": "5da59b2f-dc24-46fa-f6aa-b0bc6ae13097"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "This script is for encoding image to SAM features.\n",
            "\n",
            "=====\n",
            "Usage\n",
            "=====\n",
            "using settings.json:\n",
            "\n",
            "    image_encoder.py -s <settings.json> -f <feature_dir>\n",
            " \n",
            " \n",
            "or directly using parameters:\n",
            " \n",
            "    image_encoder.py -i <image_path> -c <checkpoint_path> -f <feature_dir>\n",
            "    \n",
            "All Parameters:\n",
            "-------------------\n",
            "-s, --settings:         Path to the settings json file.\n",
            "-i, --image_path:       Path to the input image.\n",
            "-c, --checkpoint_path:  Path to the SAM checkpoint.\n",
            "-f, --feature_dir:      Path to the output feature directory.\n",
            "--model_type: one of [\"vit_h\", \"vit_l\", \"vit_b\"] or [0, 1, 2] or None, optional\n",
            "    The type of the SAM model. If None, the model type will be \n",
            "    inferred from the checkpoint path. Default: None. \n",
            "--bands: list of int, optional .\n",
            "    The bands to be used for encoding. Should not be more than three bands.\n",
            "    If None, the first three bands (if available) will be used. Default: None.\n",
            "--stride: int, optional\n",
            "    The stride of the sliding window. Default: 512.\n",
            "--extent: str, optional\n",
            "    The extent of the image to be encoded. Should be in the format of\n",
            "    \"minx, miny, maxx, maxy, [crs]\". If None, the extent of the input\n",
            "    image will be used. Default: None.\n",
            "--value_range: tuple of float, optional\n",
            "    The value range of the input image. If None, the value range will be\n",
            "    automatically calculated from the input image. Default: None.\n",
            "--resolution: float, optional\n",
            "    The resolution of the output feature in the unit of raster crs.\n",
            "    If None, the resolution of the input image will be used. Default: None.\n",
            "--batch_size: int, optional\n",
            "    The batch size for encoding. Default: 1.\n",
            "--gpu_id: int, optional\n",
            "    The device id of the GPU to be used. Default: 0.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O3O1cA3gyUzI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}